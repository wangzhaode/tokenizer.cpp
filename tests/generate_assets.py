import os
import shutil
import json
from modelscope.hub.file_download import model_file_download
from transformers import AutoTokenizer

# ================= é…ç½®åŒºåŸŸ =================

SAVE_ROOT = "./models"

TARGET_MODELS = [
    # 1. BPE Tokenizer Models
    "Qwen/Qwen2.5-3B-Instruct",
    "Qwen/Qwen2.5-VL-3B-Instruct",
    "Qwen/Qwen2.5-Omni-3B",
    "Qwen/Qwen2.5-7B-Instruct-1M",
    "Qwen/Qwen2.5-Math-7B-Instruct",
    # "Qwen/Qwen2.5-Coder-7B-Instruct",
    "Qwen/QwQ-32B",
    "Qwen/Qwen3-4B",
    "Qwen/Qwen3-4B-Instruct-2507",
    "Qwen/Qwen3-4B-Thinking-2507",
    "Qwen/Qwen3-VL-4B-Instruct",
    "Qwen/Qwen3-VL-4B-Thinking",
    "Qwen/Qwen3Guard-Gen-4B",
    # "Qwen/Qwen3Guard-Stream-4B",
    "Qwen/Qwen3-Coder-30B-A3B-Instruct",
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    "Qwen/Qwen3-Omni-30B-A3B-Thinking",
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "deepseek-ai/DeepSeek-V3.2",
    "deepseek-ai/DeepSeek-R1",
    "ZhipuAI/GLM-4.5V",
    "ZhipuAI/GLM-4.6V",
    "HuggingFaceTB/SmolLM-135M-Instruct",
    "HuggingFaceTB/SmolVLM-256M-Instruct",
    "HuggingFaceTB/SmolLM2-135M-Instruct",
    # "HuggingFaceTB/SmolVLM2-256M-Video-Instruct",
    "HuggingFaceTB/SmolLM3-3B",
    "google/gemma-3-4b-it",
    "google/gemma-3n-E4B-it",
    "mistralai/Ministral-3-3B-Instruct-2512",
    "LLM-Research/llama-2-7b",
    "LLM-Research/Meta-Llama-3-8B-Instruct",
    "LLM-Research/Llama-3.2-3B-Instruct",
    "LLM-Research/Phi-3.5-mini-instruct",
    "LLM-Research/Phi-3.5-vision-instruct",
    "LLM-Research/phi-4",
    "LLM-Research/Phi-4-mini-reasoning",
    # # 2. WordPiece Tokenizer Models
    # "google-bert/bert-base-uncased",
    # "google-bert/bert-base-multilingual-cased",
    # "AI-ModelScope/bge-large-zh",
    # "iic/gte_sentence-embedding_multilingual-base",
    # "iic/gte-multilingual-reranker-base",
    # # 3. Unigram Tokenzier Models
    # "AI-ModelScope/t5-small",
]

CONFIG_FILES = [
    "tokenizer_config.json",
]

# ================= å¢å¼ºå‹æµ‹è¯•è¯­æ–™åº“ =================
# è¦†ç›–ï¼šåŸºç¡€è‹±æ–‡ã€å¤šè¯­è¨€ã€ä»£ç ã€Emojiã€ç©ºç™½å­—ç¬¦è¾¹ç•Œã€Unicodeè§„èŒƒåŒ–ã€Byte-level fallback

TEST_CORPUS = [
    # 1. åŸºç¡€ & æ ‡ç‚¹
    "Hello World",
    "Hello  World", # å¤šç©ºæ ¼
    " don't ",      # å‰åç©ºæ ¼ + ç¼©å†™
    "The quick brown fox jumps over the lazy dog.",

    # 2. å¤šè¯­è¨€ (CJK + æ··åˆ)
    "ä½ å¥½ä¸–ç•Œ",
    "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ", # æ—¥è¯­
    "ì•ˆë…•í•˜ì„¸ìš”",     # éŸ©è¯­
    "I love ä¸­å›½",    # ä¸­è‹±æ··åˆ
    "æ—©Cæ™šA",        # å…¸å‹æ··åˆç½‘ç»œç”¨è¯­

    # 3. ä»£ç  (å…³æ³¨ç¼©è¿›ã€æ¢è¡Œã€ç¬¦å·)
    "def main():\n    print('hello world')",
    "#include <iostream>\nusing namespace std;",
    "const a = 10; // comment",
    "print(a_b_c)",

    # 4. æ•°å­—ä¸æ•°å­¦
    "1234567890",
    "3.14159",
    "x^2 + y_2 = z",

    # 5. Emoji & ç‰¹æ®Šç¬¦å· (æµ‹è¯• Unigram/BPE å¯¹ Byte çš„å¤„ç†)
    "ğŸ˜Š ğŸ˜‚ ğŸ¥º",
    "ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦", # ç»„åˆ Emoji (ZWJ)
    "Hash#Tag $Price %Percent &And",

    # 6. è¾¹ç•Œä¸ç©ºç™½ Case (Tokenizer çš„å™©æ¢¦)
    "",             # ç©ºä¸²
    "   ",          # çº¯ç©ºæ ¼
    "\n",           # çº¯æ¢è¡Œ
    "\t\n\r",       # æ··åˆæ§åˆ¶ç¬¦
    "Hello\nWorld",

    # 7. URL & Email (é€šå¸¸ä¸åº”è¢«åˆ‡åˆ†å¾—å¤ªç¢)
    "https://github.com/google/gemini",
    "user.name@example.com",

    # 8. ç½•è§å­—ç¬¦ / Byte Fallback æµ‹è¯•
    # æŸäº› Tokenizer é‡åˆ°æœªè¯†åˆ«å­—ç¬¦ä¼šå›é€€åˆ° Byte ç¼–ç ï¼Œæˆ–è€…è¾“å‡º <unk>
    "Ãƒ", "Ã©", "Î²", "â‘ ",
]

# ================= Chat æ¨¡æ¿æµ‹è¯•è¯­æ–™ =================
# ç”¨äºæµ‹è¯• apply_chat_template çš„åŠŸèƒ½
CHAT_TEST_CORPUS = [
    # ===== 1. åŸºç¡€åœºæ™¯ =====
    {
        "name": "basic_user",
        "messages": [
            {"role": "user", "content": "Hi"}
        ]
    },
    {
        "name": "system_user_assistant",
        "messages": [
            {"role": "system", "content": "You are a helpful coding assistant specialized in Python."},
            {"role": "user", "content": "Who are you?"},
            {"role": "assistant", "content": "I am an AI assistant created to help you with Python programming."}
        ]
    },

    # ===== 2. è¾¹ç•Œåœºæ™¯ =====
    {
        "name": "consecutive_users",
        "messages": [
            {"role": "user", "content": "Part 1: What is machine learning?"},
            {"role": "user", "content": "Part 2: Give me a simple example."}
        ]
    },
    {
        "name": "gen_prompt_true",
        "messages": [{"role": "user", "content": "Hello, please help me."}],
        "add_generation_prompt": True
    },
    {
        "name": "gen_prompt_false",
        "messages": [{"role": "user", "content": "Hello, please help me."}],
        "add_generation_prompt": False
    },

    # ===== 3. å¤šè½®å¤æ‚å¯¹è¯ =====
    {
        "name": "multi_turn_code",
        "messages": [
            {"role": "system", "content": "You are a senior software engineer."},
            {"role": "user", "content": "How do I reverse a string in Python?"},
            {"role": "assistant", "content": "You can use slicing: `s[::-1]` or `''.join(reversed(s))`"},
            {"role": "user", "content": "What about for a list?"},
            {"role": "assistant", "content": "For lists: `lst[::-1]`, `list(reversed(lst))`, or `lst.reverse()` (in-place)"},
            {"role": "user", "content": "Which one is fastest?"}
        ],
        "add_generation_prompt": True
    },

    # ===== 4. å·¥å…·è°ƒç”¨åœºæ™¯ (Tool Calls) =====
    {
        "name": "tool_call_weather",
        "messages": [
            {"role": "user", "content": "What's the weather like in New York?"},
            {
                "role": "assistant",
                "content": "",
                "tool_calls": [
                    {
                        "id": "call_abc123",
                        "type": "function",
                        "function": {
                            "name": "get_weather",
                            "arguments": "{\"location\": \"New York\", \"unit\": \"celsius\"}"
                        }
                    }
                ]
            },
            {
                "role": "tool",
                "tool_call_id": "call_abc123",
                "name": "get_weather",
                "content": "{\"temperature\": 22, \"condition\": \"sunny\", \"humidity\": 45}"
            }
        ],
        "add_generation_prompt": True
    },
    {
        "name": "parallel_tool_calls",
        "messages": [
            {"role": "user", "content": "Compare weather in Tokyo and London"},
            {
                "role": "assistant",
                "content": "",
                "tool_calls": [
                    {
                        "id": "call_tokyo",
                        "type": "function",
                        "function": {"name": "get_weather", "arguments": "{\"location\": \"Tokyo\"}"}
                    },
                    {
                        "id": "call_london",
                        "type": "function",
                        "function": {"name": "get_weather", "arguments": "{\"location\": \"London\"}"}
                    }
                ]
            },
            {
                "role": "tool",
                "tool_call_id": "call_tokyo",
                "name": "get_weather",
                "content": "{\"temp\": 28, \"condition\": \"humid\"}"
            },
            {
                "role": "tool",
                "tool_call_id": "call_london",
                "name": "get_weather",
                "content": "{\"temp\": 15, \"condition\": \"rainy\"}"
            }
        ],
        "add_generation_prompt": True
    },

    # ===== 5. æ¨ç†/æ€ç»´é“¾åœºæ™¯ =====
    {
        "name": "reasoning_content",
        "messages": [
            {"role": "user", "content": "Solve: If a train travels 120km in 2 hours, what's the speed?"},
            {
                "role": "assistant",
                "content": "The speed is 60 km/h.",
                "reasoning_content": "Let me think step by step:\n1. Distance = 120 km\n2. Time = 2 hours\n3. Speed = Distance / Time = 120 / 2 = 60 km/h"
            }
        ]
    },

    # ===== 6. å¤šè¯­è¨€ä¸ç‰¹æ®Šå­—ç¬¦ =====
    {
        "name": "multilingual_complex",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¤šè¯­è¨€AIåŠ©æ‰‹ã€‚You can speak multiple languages."},
            {"role": "user", "content": "Translate 'Hello World' to: ä¸­æ–‡ã€æ—¥æœ¬èªã€í•œêµ­ì–´"},
            {"role": "assistant", "content": "Here are the translations:\n- ä¸­æ–‡: ä½ å¥½ä¸–ç•Œ\n- æ—¥æœ¬èª: ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ\n- í•œêµ­ì–´: ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„"}
        ]
    },
    {
        "name": "escape_characters",
        "messages": [
            {"role": "user", "content": "Explain these escape sequences: \\n \\t \\r \\\\ \\\""},
            {"role": "assistant", "content": "Here's what each means:\n- \\n = newline\n- \\t = tab\n- \\r = carriage return\n- \\\\ = backslash\n- \\\" = double quote"}
        ]
    },
    {
        "name": "code_with_special_chars",
        "messages": [
            {"role": "user", "content": "Write a regex to match email addresses"},
            {"role": "assistant", "content": "```python\nimport re\npattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\nemail = 'user@example.com'\nif re.match(pattern, email):\n    print('Valid!')\n```"}
        ]
    },

    # ===== 7. é•¿æ–‡æœ¬ä¸ Markdown =====
    {
        "name": "markdown_formatting",
        "messages": [
            {"role": "user", "content": "Show me markdown formatting examples"},
            {"role": "assistant", "content": "# Heading 1\n## Heading 2\n\n**Bold** and *italic* text.\n\n- Bullet point 1\n- Bullet point 2\n\n```python\ndef hello():\n    return 'world'\n```\n\n| Col1 | Col2 |\n|------|------|\n| A    | B    |\n|------|------|\n| C    | D    |"}
        ]
    },

    # ===== 8. ç©ºå†…å®¹ä¸è¾¹ç•Œ =====
    {
        "name": "empty_assistant_content",
        "messages": [
            {"role": "user", "content": "Say nothing"},
            {"role": "assistant", "content": ""}
        ]
    },
    {
        "name": "whitespace_only",
        "messages": [
            {"role": "user", "content": "   \n\t   "}
        ]
    },

    # ===== 9. Emoji ä¸ Unicode =====
    {
        "name": "emoji_conversation",
        "messages": [
            {"role": "user", "content": "Respond with emojis only: how are you?"},
            {"role": "assistant", "content": "ğŸ˜ŠğŸ‘âœ¨ğŸ‰"}
        ]
    },
    {
        "name": "unicode_math_symbols",
        "messages": [
            {"role": "user", "content": "Write the quadratic formula using Unicode"},
            {"role": "assistant", "content": "x = (-b Â± âˆš(bÂ² - 4ac)) / 2a\n\nOr in fancy form:\nğ‘¥ = (âˆ’ğ‘ Â± âˆš(ğ‘Â² âˆ’ 4ğ‘ğ‘)) / 2ğ‘"}
        ]
    },

    # ===== 10. æ—¥æœŸæ³¨å…¥æ¨¡æ‹Ÿ =====
    {
        "name": "date_injection",
        "messages": [
            {"role": "system", "content": "Current Date: 2025-12-17. You are a calendar assistant."},
            {"role": "user", "content": "What's today's date?"},
            {"role": "assistant", "content": "Today is December 17, 2025."}
        ]
    },

    # ===== 11. JSON å†…å®¹ =====
    {
        "name": "json_in_content",
        "messages": [
            {"role": "user", "content": "Parse this JSON: {\"name\": \"John\", \"age\": 30, \"skills\": [\"python\", \"javascript\"]}"},
            {"role": "assistant", "content": "The JSON contains:\n- name: John\n- age: 30\n- skills: python, javascript"}
        ]
    },

    # ===== 12. è¶…é•¿ç³»ç»Ÿæç¤º =====
    {
        "name": "long_system_prompt",
        "messages": [
            {"role": "system", "content": "You are an expert AI assistant with the following capabilities:\n1. Code review and debugging\n2. Algorithm design and optimization\n3. System architecture consultation\n4. Technical documentation writing\n5. Best practices recommendation\n\nRules:\n- Always provide detailed explanations\n- Include code examples when relevant\n- Consider edge cases\n- Follow security best practices"},
            {"role": "user", "content": "Review my code"}
        ],
        "add_generation_prompt": True
    },
]

# ================= åŠŸèƒ½å‡½æ•° =================

def generate_test_cases(tokenizer, output_dir):
    """
    ä½¿ç”¨åŠ è½½å¥½çš„ tokenizer ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
    åŒ…å«:
    1. basic ç±»å‹: åŸºç¡€ tokenization æµ‹è¯•
    2. chat  ç±»å‹: apply_chat_template æµ‹è¯•
    """
    cases_path = os.path.join(output_dir, "test_cases.jsonl")
    print(f"  ğŸ§ª ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ -> {cases_path}")

    with open(cases_path, "w", encoding="utf-8") as f:
        # ===== 1. åŸºç¡€ Tokenization æµ‹è¯• =====
        for text in TEST_CORPUS:
            try:
                # 1. çº¯åˆ†è¯æ¨¡å¼ (add_special_tokens=False)
                enc_raw = tokenizer(text, add_special_tokens=False)
                tokens_raw = tokenizer.convert_ids_to_tokens(enc_raw["input_ids"])

                # 2. å®Œæ•´æ¨¡å¼ (add_special_tokens=True)
                enc_full = tokenizer(text, add_special_tokens=True)

                record = {
                    "type": "basic",
                    "input": text,
                    "ids_raw": enc_raw["input_ids"],
                    "tokens_raw": tokens_raw,
                    "ids_full": enc_full["input_ids"],
                }

                f.write(json.dumps(record, ensure_ascii=False) + "\n")

            except Exception as e:
                print(f"    âš ï¸ Basic Case Error '{text}': {e}")

        # ===== 2. Chat Template æµ‹è¯• =====
        # æ£€æŸ¥ tokenizer æ˜¯å¦æ”¯æŒ chat_template
        if not hasattr(tokenizer, 'apply_chat_template') or tokenizer.chat_template is None:
            print(f"    âš ï¸ Tokenizer ä¸æ”¯æŒ chat_templateï¼Œè·³è¿‡ chat æµ‹è¯•")
            return

        for chat_case in CHAT_TEST_CORPUS:
            try:
                messages = chat_case["messages"]
                add_generation_prompt = chat_case.get("add_generation_prompt", False)

                # ä½¿ç”¨ apply_chat_template ç”Ÿæˆæ ¼å¼åŒ–æ–‡æœ¬
                formatted_text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=add_generation_prompt
                )

                # å¯¹æ ¼å¼åŒ–æ–‡æœ¬è¿›è¡Œ tokenization
                enc_ids = tokenizer.apply_chat_template(
                    messages,
                    tokenize=True,
                    add_generation_prompt=add_generation_prompt,
                    return_tensors=None  # è¿”å› list è€Œé tensor
                )

                record = {
                    "type": "chat",
                    "name": chat_case["name"],
                    "messages": messages,
                    "add_generation_prompt": add_generation_prompt,
                    "formatted_text": formatted_text,
                    "ids": enc_ids,
                }

                f.write(json.dumps(record, ensure_ascii=False) + "\n")

            except Exception as e:
                print(f"    âš ï¸ Chat Case Error '{chat_case['name']}': {e}")

def process_tokenizers():
    print(f"ğŸš€ å¼€å§‹å¤„ç† Tokenizer æ•°æ®ä¸æµ‹è¯•é›† (å…± {len(TARGET_MODELS)} ä¸ªæ¨¡å‹)\n")

    for idx, model_id in enumerate(TARGET_MODELS, 1):
        folder_name = model_id.split("/")[-1]
        local_dir = os.path.join(SAVE_ROOT, folder_name)
        os.makedirs(local_dir, exist_ok=True)

        print(f"[{idx}/{len(TARGET_MODELS)}] å¤„ç†: {model_id}")

        # --- æ­¥éª¤ 1: ä¸‹è½½ Config æ–‡ä»¶ ---
        for filename in CONFIG_FILES:
            try:
                cached_path = model_file_download(model_id=model_id, file_path=filename, revision='master')
                shutil.copy(cached_path, os.path.join(local_dir, filename))
            except Exception:
                pass

        # --- æ­¥éª¤ 2: å‡†å¤‡ tokenizer.json ---
        target_json_path = os.path.join(local_dir, "tokenizer.json")
        json_ready = False

        # 2.1 å°è¯•ç›´æ¥ä¸‹è½½
        try:
            cached_path = model_file_download(model_id=model_id, file_path="tokenizer.json", revision='master')
            shutil.copy(cached_path, target_json_path)
            # print(f"  âœ… [Download] tokenizer.json")
            json_ready = True
        except Exception:
            pass

        # 2.2 ä¸‹è½½å¤±è´¥åˆ™è½¬æ¢ (é’ˆå¯¹ BERT ç­‰åªæœ‰ vocab.txt çš„æ¨¡å‹)
        if not json_ready:
            try:
                print(f"  ğŸ”„ [Convert] æ­£åœ¨ä»åŸå§‹æ¨¡å‹è½¬æ¢ Fast Tokenizer...")
                # trust_remote_code=True å…è®¸æ‰§è¡Œæ¨¡å‹ä»“åº“é‡Œçš„ Python ä»£ç  (å¯¹ Qwen/GLM å¿…é¡»)
                temp_tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
                if temp_tokenizer.is_fast:
                    temp_tokenizer.backend_tokenizer.save(target_json_path)
                    print(f"  âœ… [Saved] å·²ç”Ÿæˆ tokenizer.json")
                    json_ready = True
                else:
                    print(f"  âŒ [Error] æ¨¡å‹ä¸æ”¯æŒ Fast Tokenizer")
            except Exception as e:
                print(f"  âŒ [Error] è½¬æ¢å¤±è´¥: {str(e)[:100]}")

        # --- æ­¥éª¤ 3: é‡æ–°åŠ è½½æœ¬åœ°æ¨¡å‹å¹¶ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ ---
        if json_ready:
            try:
                # å…³é”®ï¼šä»ã€æœ¬åœ°ç›®å½•ã€‘åŠ è½½ Tokenizer
                # è¿™æ ·ä¿è¯ç”Ÿæˆçš„æµ‹è¯•æ•°æ®ä¸ç£ç›˜ä¸Šçš„ tokenizer.json ç»å¯¹ä¸€è‡´
                # é¿å…å†…å­˜ä¸­çš„ tokenizer ä¸ç£ç›˜æ–‡ä»¶ç‰ˆæœ¬ä¸ä¸€è‡´çš„æƒ…å†µ
                local_tokenizer = AutoTokenizer.from_pretrained(local_dir, trust_remote_code=True)

                # ç”Ÿæˆæµ‹è¯•æ•°æ®
                generate_test_cases(local_tokenizer, local_dir)

            except Exception as e:
                print(f"  âŒ [Error] æœ¬åœ°åŠ è½½æˆ–ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        else:
            # æ¸…ç†æ— æ•ˆç›®å½•
            if os.path.exists(local_dir) and not os.listdir(local_dir):
                os.rmdir(local_dir)

        print("-" * 40)

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆ! æ•°æ®ä¿å­˜åœ¨: {os.path.abspath(SAVE_ROOT)}")

if __name__ == "__main__":
    process_tokenizers()